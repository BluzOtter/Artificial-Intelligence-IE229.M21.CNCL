{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Problem2_Labwork5.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1 - RNN + LSTM"
      ],
      "metadata": {
        "id": "MZ3LBcQV3kZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import lib"
      ],
      "metadata": {
        "id": "THZh-Y7G4wB_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "import time\n",
        "import random\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "NPK-zEGK4X7B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1"
      ],
      "metadata": {
        "id": "EwwxvrS74yuJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "caAeafoh42Eu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
        "                                          split_ratio=0.8)\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GfE_1v-G40lc",
        "outputId": "a4ced165-c090-440c-8a1f-c144fd26f887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 20000\n",
            "Num Valid: 5000\n",
            "Num Test: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LAr-xJ1E452G",
        "outputId": "7400607b-52b6-4f97-c7f1-6bd17b1e5e03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20002\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    device=DEVICE)"
      ],
      "metadata": {
        "id": "frYGVESJ48Vb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9lyAf2p25FPV",
        "outputId": "3ce0362d-624d-492a-d03c-d6cfe53f812e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([132, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([59, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([42, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "uQjYqB0g5IBS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
        "        \n",
        "        #[sentence len, batch size, embedding size] => \n",
        "        #  output: [sentence len, batch size, hidden size]\n",
        "        #  hidden: [1, batch size, hidden size]\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0)).view(-1)"
      ],
      "metadata": {
        "id": "57a-YGHA5JsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "BUsSITJv5SKc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "WB8C7DP55UYq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.text\n",
        "            logits = model(text, text_lengths)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.label.size(0)\n",
        "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "h9hHiOnR5SzD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.text\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t0b6asGn5Xiy",
        "outputId": "f47bb705-79d3-4b13-b06f-301400421455"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/015 | Batch 000/157 | Cost: 0.6926\n",
            "Epoch: 001/015 | Batch 050/157 | Cost: 0.6961\n",
            "Epoch: 001/015 | Batch 100/157 | Cost: 0.6822\n",
            "Epoch: 001/015 | Batch 150/157 | Cost: 0.6801\n",
            "training accuracy: 57.45%\n",
            "valid accuracy: 55.80%\n",
            "Time elapsed: 0.21 min\n",
            "Epoch: 002/015 | Batch 000/157 | Cost: 0.6784\n",
            "Epoch: 002/015 | Batch 050/157 | Cost: 0.6727\n",
            "Epoch: 002/015 | Batch 100/157 | Cost: 0.5835\n",
            "Epoch: 002/015 | Batch 150/157 | Cost: 0.6063\n",
            "training accuracy: 70.53%\n",
            "valid accuracy: 70.06%\n",
            "Time elapsed: 0.39 min\n",
            "Epoch: 003/015 | Batch 000/157 | Cost: 0.5987\n",
            "Epoch: 003/015 | Batch 050/157 | Cost: 0.5703\n",
            "Epoch: 003/015 | Batch 100/157 | Cost: 0.5279\n",
            "Epoch: 003/015 | Batch 150/157 | Cost: 0.5966\n",
            "training accuracy: 75.06%\n",
            "valid accuracy: 73.86%\n",
            "Time elapsed: 0.57 min\n",
            "Epoch: 004/015 | Batch 000/157 | Cost: 0.4822\n",
            "Epoch: 004/015 | Batch 050/157 | Cost: 0.4943\n",
            "Epoch: 004/015 | Batch 100/157 | Cost: 0.4635\n",
            "Epoch: 004/015 | Batch 150/157 | Cost: 0.5266\n",
            "training accuracy: 78.39%\n",
            "valid accuracy: 76.46%\n",
            "Time elapsed: 0.77 min\n",
            "Epoch: 005/015 | Batch 000/157 | Cost: 0.5138\n",
            "Epoch: 005/015 | Batch 050/157 | Cost: 0.4163\n",
            "Epoch: 005/015 | Batch 100/157 | Cost: 0.4524\n",
            "Epoch: 005/015 | Batch 150/157 | Cost: 0.4805\n",
            "training accuracy: 80.18%\n",
            "valid accuracy: 78.56%\n",
            "Time elapsed: 0.95 min\n",
            "Epoch: 006/015 | Batch 000/157 | Cost: 0.4918\n",
            "Epoch: 006/015 | Batch 050/157 | Cost: 0.3926\n",
            "Epoch: 006/015 | Batch 100/157 | Cost: 0.4835\n",
            "Epoch: 006/015 | Batch 150/157 | Cost: 0.5255\n",
            "training accuracy: 74.49%\n",
            "valid accuracy: 72.68%\n",
            "Time elapsed: 1.14 min\n",
            "Epoch: 007/015 | Batch 000/157 | Cost: 0.4905\n",
            "Epoch: 007/015 | Batch 050/157 | Cost: 0.4019\n",
            "Epoch: 007/015 | Batch 100/157 | Cost: 0.4512\n",
            "Epoch: 007/015 | Batch 150/157 | Cost: 0.3053\n",
            "training accuracy: 83.65%\n",
            "valid accuracy: 80.24%\n",
            "Time elapsed: 1.32 min\n",
            "Epoch: 008/015 | Batch 000/157 | Cost: 0.3934\n",
            "Epoch: 008/015 | Batch 050/157 | Cost: 0.3865\n",
            "Epoch: 008/015 | Batch 100/157 | Cost: 0.2878\n",
            "Epoch: 008/015 | Batch 150/157 | Cost: 0.3724\n",
            "training accuracy: 85.47%\n",
            "valid accuracy: 82.12%\n",
            "Time elapsed: 1.50 min\n",
            "Epoch: 009/015 | Batch 000/157 | Cost: 0.3785\n",
            "Epoch: 009/015 | Batch 050/157 | Cost: 0.4805\n",
            "Epoch: 009/015 | Batch 100/157 | Cost: 0.3006\n",
            "Epoch: 009/015 | Batch 150/157 | Cost: 0.3836\n",
            "training accuracy: 85.71%\n",
            "valid accuracy: 82.64%\n",
            "Time elapsed: 1.68 min\n",
            "Epoch: 010/015 | Batch 000/157 | Cost: 0.4089\n",
            "Epoch: 010/015 | Batch 050/157 | Cost: 0.3634\n",
            "Epoch: 010/015 | Batch 100/157 | Cost: 0.3659\n",
            "Epoch: 010/015 | Batch 150/157 | Cost: 0.4318\n",
            "training accuracy: 86.47%\n",
            "valid accuracy: 82.08%\n",
            "Time elapsed: 1.86 min\n",
            "Epoch: 011/015 | Batch 000/157 | Cost: 0.3098\n",
            "Epoch: 011/015 | Batch 050/157 | Cost: 0.3464\n",
            "Epoch: 011/015 | Batch 100/157 | Cost: 0.2843\n",
            "Epoch: 011/015 | Batch 150/157 | Cost: 0.3018\n",
            "training accuracy: 87.12%\n",
            "valid accuracy: 82.94%\n",
            "Time elapsed: 2.04 min\n",
            "Epoch: 012/015 | Batch 000/157 | Cost: 0.3330\n",
            "Epoch: 012/015 | Batch 050/157 | Cost: 0.2543\n",
            "Epoch: 012/015 | Batch 100/157 | Cost: 0.3131\n",
            "Epoch: 012/015 | Batch 150/157 | Cost: 0.3131\n",
            "training accuracy: 88.50%\n",
            "valid accuracy: 83.88%\n",
            "Time elapsed: 2.22 min\n",
            "Epoch: 013/015 | Batch 000/157 | Cost: 0.3078\n",
            "Epoch: 013/015 | Batch 050/157 | Cost: 0.2364\n",
            "Epoch: 013/015 | Batch 100/157 | Cost: 0.3307\n",
            "Epoch: 013/015 | Batch 150/157 | Cost: 0.3450\n",
            "training accuracy: 88.98%\n",
            "valid accuracy: 84.46%\n",
            "Time elapsed: 2.41 min\n",
            "Epoch: 014/015 | Batch 000/157 | Cost: 0.3768\n",
            "Epoch: 014/015 | Batch 050/157 | Cost: 0.3199\n",
            "Epoch: 014/015 | Batch 100/157 | Cost: 0.2975\n",
            "Epoch: 014/015 | Batch 150/157 | Cost: 0.2078\n",
            "training accuracy: 89.34%\n",
            "valid accuracy: 85.16%\n",
            "Time elapsed: 2.59 min\n",
            "Epoch: 015/015 | Batch 000/157 | Cost: 0.3260\n",
            "Epoch: 015/015 | Batch 050/157 | Cost: 0.2405\n",
            "Epoch: 015/015 | Batch 100/157 | Cost: 0.2861\n",
            "Epoch: 015/015 | Batch 150/157 | Cost: 0.3008\n",
            "training accuracy: 90.47%\n",
            "valid accuracy: 84.68%\n",
            "Time elapsed: 2.78 min\n",
            "Total Training Time: 2.78 min\n",
            "Test accuracy: 84.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 - RNN + LSTM + Glob"
      ],
      "metadata": {
        "id": "mU0KnYGJaRSs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install lib and import lib"
      ],
      "metadata": {
        "id": "tDxx8Nq0xyvG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchdata"
      ],
      "metadata": {
        "id": "RVeWThPeFlbX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torchtext==0.10.0"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QOgJvjDZuJBg",
        "outputId": "d82b6850-05ce-49c9-d48c-87c3e7cbf9b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting torchtext==0.10.0\n",
            "  Downloading torchtext-0.10.0-cp37-cp37m-manylinux1_x86_64.whl (7.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 7.6 MB 7.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (4.64.0)\n",
            "Collecting torch==1.9.0\n",
            "  Downloading torch-1.9.0-cp37-cp37m-manylinux1_x86_64.whl (831.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 831.4 MB 2.7 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext==0.10.0) (1.21.6)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.9.0->torchtext==0.10.0) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext==0.10.0) (1.24.3)\n",
            "Installing collected packages: torch, torchtext\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 1.11.0+cu113\n",
            "    Uninstalling torch-1.11.0+cu113:\n",
            "      Successfully uninstalled torch-1.11.0+cu113\n",
            "  Attempting uninstall: torchtext\n",
            "    Found existing installation: torchtext 0.12.0\n",
            "    Uninstalling torchtext-0.12.0:\n",
            "      Successfully uninstalled torchtext-0.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchvision 0.12.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\n",
            "torchaudio 0.11.0+cu113 requires torch==1.11.0, but you have torch 1.9.0 which is incompatible.\u001b[0m\n",
            "Successfully installed torch-1.9.0 torchtext-0.10.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchtext\n",
        "import random\n",
        "\n",
        "from torchtext.legacy import data\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "import random"
      ],
      "metadata": {
        "id": "wJ4MDnIdv3B2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "MODELNAME = \"imdb-rnn.model\"\n",
        "NUM_EPOCHS = 15\n",
        "# BATCHSIZE = 64\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "HIDDEN_DIM = 256\n",
        "OUTPUT_DIM = 1"
      ],
      "metadata": {
        "id": "OqDww3HMH_4j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data\n",
        "\n",
        "One of the main concepts of TorchText is the `Field`. These define how your data should be processed. In our sentiment classification task the data consists of both the raw string of the review and the sentiment, either \"pos\" or \"neg\".\n",
        "\n",
        "The parameters of a `Field` specify how the data should be processed. \n",
        "\n",
        "We use the `TEXT` field to define how the review should be processed, and the `LABEL` field to process the sentiment. \n",
        "\n",
        "Our `TEXT` field has `tokenize='spacy'` as an argument. This defines that the \"tokenization\" (the act of splitting the string into discrete \"tokens\") should be done using the [spaCy](https://spacy.io) tokenizer. If no `tokenize` argument is passed, the default is simply splitting the string on spaces. We also need to specify a `tokenizer_language` which tells torchtext which spaCy model to use. We use the `en_core_web_sm` model which has to be downloaded with `python -m spacy download en_core_web_sm` before you run this notebook!\n",
        "\n",
        "`LABEL` is defined by a `LabelField`, a special subset of the `Field` class specifically used for handling labels. We will explain the `dtype` argument later.\n",
        "\n",
        "For more on `Fields`, go [here](https://github.com/pytorch/text/blob/master/torchtext/data/field.py).\n",
        "\n",
        "We also set the random seeds for reproducibility. "
      ],
      "metadata": {
        "id": "d1GEcH-0x9We"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vjgx0Obt-X8R",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5986dc25-e647-4421-9c96-6c9269c89b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:03<00:00, 23.5MB/s]\n"
          ]
        }
      ],
      "source": [
        "TEXT = data.Field(tokenize='spacy', include_lengths=True) # necessary for packed_padded_sequence\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
        "                                          split_ratio=0.8)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# train_data = [(label, tokenizer(line)) for label, line in train_iter]\n",
        "# train_data.sort(key = lambda x: len(x[1]))\n",
        "# test_data = [(label, tokenizer(line)) for label, line in test_iter]\n",
        "# test_data.sort(key = lambda x: len(x[1]))\n",
        "# for i in range(10):\n",
        "  # print(train_data[i])"
      ],
      "metadata": {
        "id": "W5MERhtDI1nv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DhJJ3iOCv84F",
        "outputId": "b61f613c-6c2f-44a6-eca2-c944ce36ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 20000\n",
            "Num Valid: 5000\n",
            "Num Test: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Build the vocabulary based on the top \"VOCABULARY_SIZE\" words:\n",
        "\n",
        "Text.Vocab Dictionary sẽ chứa số lượng từ và chỉ số từ. Lý do tại sao số lượng từ là VOCABULARY_SIZE + 2 là vì nó chứa các mã thông báo đặc biệt để đệm và các từ chưa biết: <'unk> and <'pad>."
      ],
      "metadata": {
        "id": "yjTvmdvUxs8X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KTmxtUgcxwAW",
        "outputId": "b4d955d3-43d9-4b9b-bdc3-858b339853ac"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:39, 5.39MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29464.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20002\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    device=DEVICE)"
      ],
      "metadata": {
        "id": "ItP6AAvEvErM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LiITJzt20EOq",
        "outputId": "c7a294ed-f371-44c3-f113-7ce4f5de6ee3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([132, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([59, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([42, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_vocab(train_data, min_freq):\n",
        "#   vocab = {}\n",
        "#   for label, tokenlist in train_data:\n",
        "#     for token in tokenlist:\n",
        "#       if token not in vocab:\n",
        "#         vocab[token] = 0\n",
        "#       vocab[token] += 1\n",
        "#   vocablist = [('<unk>', 0), ('<pad>', 0), ('<cls>', 0), ('<eos>', 0)]\n",
        "#   vocabidx = {}\n",
        "#   for token, freq in vocab.items():\n",
        "#     if freq >= min_freq:\n",
        "#       idx = len(vocablist)\n",
        "#       vocablist.append((token, freq))\n",
        "#       vocabidx[token]=idx\n",
        "#   vocabidx['<unk>']=0\n",
        "#   vocabidx['<pad>']=1\n",
        "#   vocabidx['<cls>']=2\n",
        "#   vocabidx['<eos>']=3\n",
        "#   return vocablist, vocabidx\n",
        "\n",
        "# vocablist, vocabidx = make_vocab(train_data, 10)"
      ],
      "metadata": {
        "id": "n4u30kL4K5L3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def preprocess(data, vocabidx):\n",
        "#   rr = []\n",
        "#   for label, tokenlist in data:\n",
        "#     tkl = ['<cls>']\n",
        "#     for token in tokenlist:\n",
        "#       tkl.append(token if token in vocabidx else '<unk>')\n",
        "#     tkl.append('<eos>')\n",
        "#     rr.append((label, tkl))\n",
        "#   return rr\n",
        "\n",
        "# train_data = preprocess(train_data, vocabidx)\n",
        "# test_data = preprocess(test_data, vocabidx)\n",
        "# for i in range(10):\n",
        "#   print(train_data[i])"
      ],
      "metadata": {
        "id": "2dt34I-eSqbq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def make_batch(data, batchsize):\n",
        "#   bb = []\n",
        "#   blabel = []\n",
        "#   btokenlist = []\n",
        "#   for label, tokenlist in data: \n",
        "#     blabel.append(label)\n",
        "#     btokenlist.append(tokenlist)\n",
        "#     if len(blabel) >= batchsize:\n",
        "#       bb.append((btokenlist, blabel))\n",
        "#       blabel = []\n",
        "#       btokenlist = []\n",
        "#   if len(blabel) > 0:\n",
        "#     bb.append((btokenlist, blabel))\n",
        "#   return bb\n",
        "\n",
        "# train_data = make_batch(train_data, BATCHSIZE)\n",
        "# test_data = make_batch(test_data, BATCHSIZE)\n",
        "# for i in range(10):\n",
        "#   print(train_data[i])"
      ],
      "metadata": {
        "id": "lQPxPwUfNKJV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def padding(bb):\n",
        "#   for tokenlists, labels in bb: \n",
        "#     maxlen = max([len(x) for x in tokenlists])\n",
        "#     for tkl in tokenlists:\n",
        "#       for i in range(maxlen - len(tkl)):\n",
        "#         tkl.append('<pad>')\n",
        "#   return bb\n",
        "\n",
        "# train_data = padding(train_data)\n",
        "# test_data = padding(test_data)\n",
        "# for i in range(10):\n",
        "#   print(train_data[i])"
      ],
      "metadata": {
        "id": "Urk_VEnIOOk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def word2id(bb, vocabidx):\n",
        "#   rr = []\n",
        "#   for tokenlists, labels in bb:\n",
        "#     id_labels = [1 if label == 'pos' else 0 for label in labels]\n",
        "#     id_tokenlists = []\n",
        "#     for tokenlist in tokenlists:\n",
        "#       id_tokenlists.append([vocabidx[token] for token in tokenlist])\n",
        "#     rr.append((id_tokenlists, id_labels))\n",
        "#   return rr\n",
        "\n",
        "# train_data = word2id(train_data, vocabidx)\n",
        "# test_data = word2id(test_data, vocabidx)\n",
        "# for i in range(10):\n",
        "#   print(train_data[i])"
      ],
      "metadata": {
        "id": "dDImuOUEPIDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "MbK02WY40VOT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# class MyRNN(torch.nn.Module):\n",
        "#   def __init__(self):\n",
        "#     super(MyRNN, self).__init__()\n",
        "#     vocabsize = len(vocablist)\n",
        "#     self.emb = torch.nn.Embedding(vocabsize, 300, padding_idx=vocabidx['<pad>'])\n",
        "#     self.l1 = torch.nn.Linear(300,300)\n",
        "#     self.l2 = torch.nn.Linear(300, 2)\n",
        "#   def forward(self, x):\n",
        "#     e = self.emb(x)\n",
        "#     h = torch.zeros(e[0].size(),dtype=torch.float32).to(DEVICE)\n",
        "#     for i in range(x.size()[0]):\n",
        "#       h = F.relu(e[i] + self.l1(h))\n",
        "#     return self.l2(h)"
      ],
      "metadata": {
        "id": "EyQ-MtHSPEqX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = MyRNN().to(DEVICE)\n",
        "# print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g2EunqYVVn-s",
        "outputId": "75afb18b-e3f8-4810-c766-2309badca954"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MyRNN(\n",
            "  (emb): Embedding(20439, 300, padding_idx=1)\n",
            "  (l1): Linear(in_features=300, out_features=300, bias=True)\n",
            "  (l2): Linear(in_features=300, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim, hidden_dim)\n",
        "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
        "        \n",
        "        #[sentence len, batch size, embedding size] => \n",
        "        #  output: [sentence len, batch size, hidden size]\n",
        "        #  hidden: [1, batch size, hidden size]\n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        return self.fc(hidden.squeeze(0)).view(-1)"
      ],
      "metadata": {
        "id": "s2A4WWwE01XI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "884Vx47P1LLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rds9OlM01Nht",
        "outputId": "e835a325-fc43-4059-d1bd-f2bbdfb46c10"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RNN(\n",
            "  (embedding): Embedding(20002, 128)\n",
            "  (rnn): LSTM(128, 256)\n",
            "  (fc): Linear(in_features=256, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model "
      ],
      "metadata": {
        "id": "T8859NId0kdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def train():\n",
        "#   model = MyRNN().to(DEVICE)\n",
        "#   optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
        "#   for epoch in range(EPOCH):\n",
        "#     loss = 0\n",
        "#     for tokenlists, labels in train_data:\n",
        "#       tokenlists = torch.tensor(tokenlists, dtype=torch.int64).transpose(0, 1).to(DEVICE)\n",
        "#       labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)\n",
        "#       optimizer.zero_grad()\n",
        "#       y = model(tokenlists)\n",
        "#       print(len(y))\n",
        "#       batchloss = F.cross_entropy(y, labels)\n",
        "#       batchloss.backward()\n",
        "#       optimizer.step()\n",
        "#       loss = loss + batchloss.item()\n",
        "#     print(\"epoch\", epoch, \": loss\")\n",
        "#   torch.save(model.state_dict(), MODELNAME)\n",
        "\n",
        "# train()"
      ],
      "metadata": {
        "id": "fD9mQj_eN14f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def test():\n",
        "#   total = 0\n",
        "#   correct = 0\n",
        "#   model = MyRNN().to(DEVICE)\n",
        "#   model.load_state_dict(torch.load(MODELNAME))\n",
        "#   model.eval()\n",
        "#   for tokenlists, labels in test_data:\n",
        "#     total += len(labels)\n",
        "#     tokenlists = torch.tensor(tokenlists, dtype=torch.int64).transpose(0, 1).to(DEVICE)\n",
        "#     labels = torch.tensor(labels, dtype=torch.int64).to(DEVICE)\n",
        "#     y = model(tokenlists)\n",
        "#     pred_labels = y.max(dim=1)[1]\n",
        "#     correct += (pred_labels == labels).sum()\n",
        "#   print(\"correct:\", correct.item())\n",
        "#   print(\"total:\", total)\n",
        "#   print(\"accuracy:\", (correct.item() / float(total)))"
      ],
      "metadata": {
        "id": "BthcajyhMmIg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train()\n",
        "# test()"
      ],
      "metadata": {
        "id": "IXLz_fkELQAR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.text\n",
        "            logits = model(text, text_lengths)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.label.size(0)\n",
        "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "F47lahwUXsIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.text\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "id": "UcaTwLSLS_t4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "268ff914-9de2-4d44-9dbf-c2bc214b03a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/015 | Batch 000/157 | Cost: 0.6926\n",
            "Epoch: 001/015 | Batch 050/157 | Cost: 0.6961\n",
            "Epoch: 001/015 | Batch 100/157 | Cost: 0.6822\n",
            "Epoch: 001/015 | Batch 150/157 | Cost: 0.6801\n",
            "training accuracy: 57.45%\n",
            "valid accuracy: 55.80%\n",
            "Time elapsed: 0.18 min\n",
            "Epoch: 002/015 | Batch 000/157 | Cost: 0.6784\n",
            "Epoch: 002/015 | Batch 050/157 | Cost: 0.6727\n",
            "Epoch: 002/015 | Batch 100/157 | Cost: 0.5835\n",
            "Epoch: 002/015 | Batch 150/157 | Cost: 0.6063\n",
            "training accuracy: 70.53%\n",
            "valid accuracy: 70.06%\n",
            "Time elapsed: 0.35 min\n",
            "Epoch: 003/015 | Batch 000/157 | Cost: 0.5987\n",
            "Epoch: 003/015 | Batch 050/157 | Cost: 0.5703\n",
            "Epoch: 003/015 | Batch 100/157 | Cost: 0.5279\n",
            "Epoch: 003/015 | Batch 150/157 | Cost: 0.5966\n",
            "training accuracy: 75.06%\n",
            "valid accuracy: 73.86%\n",
            "Time elapsed: 0.53 min\n",
            "Epoch: 004/015 | Batch 000/157 | Cost: 0.4822\n",
            "Epoch: 004/015 | Batch 050/157 | Cost: 0.4943\n",
            "Epoch: 004/015 | Batch 100/157 | Cost: 0.4635\n",
            "Epoch: 004/015 | Batch 150/157 | Cost: 0.5266\n",
            "training accuracy: 78.39%\n",
            "valid accuracy: 76.46%\n",
            "Time elapsed: 0.71 min\n",
            "Epoch: 005/015 | Batch 000/157 | Cost: 0.5138\n",
            "Epoch: 005/015 | Batch 050/157 | Cost: 0.4163\n",
            "Epoch: 005/015 | Batch 100/157 | Cost: 0.4524\n",
            "Epoch: 005/015 | Batch 150/157 | Cost: 0.4805\n",
            "training accuracy: 80.18%\n",
            "valid accuracy: 78.56%\n",
            "Time elapsed: 0.89 min\n",
            "Epoch: 006/015 | Batch 000/157 | Cost: 0.4918\n",
            "Epoch: 006/015 | Batch 050/157 | Cost: 0.3926\n",
            "Epoch: 006/015 | Batch 100/157 | Cost: 0.4835\n",
            "Epoch: 006/015 | Batch 150/157 | Cost: 0.5255\n",
            "training accuracy: 74.49%\n",
            "valid accuracy: 72.68%\n",
            "Time elapsed: 1.07 min\n",
            "Epoch: 007/015 | Batch 000/157 | Cost: 0.4905\n",
            "Epoch: 007/015 | Batch 050/157 | Cost: 0.4019\n",
            "Epoch: 007/015 | Batch 100/157 | Cost: 0.4512\n",
            "Epoch: 007/015 | Batch 150/157 | Cost: 0.3053\n",
            "training accuracy: 83.65%\n",
            "valid accuracy: 80.24%\n",
            "Time elapsed: 1.25 min\n",
            "Epoch: 008/015 | Batch 000/157 | Cost: 0.3934\n",
            "Epoch: 008/015 | Batch 050/157 | Cost: 0.3865\n",
            "Epoch: 008/015 | Batch 100/157 | Cost: 0.2878\n",
            "Epoch: 008/015 | Batch 150/157 | Cost: 0.3724\n",
            "training accuracy: 85.47%\n",
            "valid accuracy: 82.12%\n",
            "Time elapsed: 1.43 min\n",
            "Epoch: 009/015 | Batch 000/157 | Cost: 0.3785\n",
            "Epoch: 009/015 | Batch 050/157 | Cost: 0.4805\n",
            "Epoch: 009/015 | Batch 100/157 | Cost: 0.3006\n",
            "Epoch: 009/015 | Batch 150/157 | Cost: 0.3836\n",
            "training accuracy: 85.71%\n",
            "valid accuracy: 82.64%\n",
            "Time elapsed: 1.61 min\n",
            "Epoch: 010/015 | Batch 000/157 | Cost: 0.4089\n",
            "Epoch: 010/015 | Batch 050/157 | Cost: 0.3634\n",
            "Epoch: 010/015 | Batch 100/157 | Cost: 0.3659\n",
            "Epoch: 010/015 | Batch 150/157 | Cost: 0.4318\n",
            "training accuracy: 86.47%\n",
            "valid accuracy: 82.08%\n",
            "Time elapsed: 1.79 min\n",
            "Epoch: 011/015 | Batch 000/157 | Cost: 0.3098\n",
            "Epoch: 011/015 | Batch 050/157 | Cost: 0.3464\n",
            "Epoch: 011/015 | Batch 100/157 | Cost: 0.2843\n",
            "Epoch: 011/015 | Batch 150/157 | Cost: 0.3018\n",
            "training accuracy: 87.12%\n",
            "valid accuracy: 82.94%\n",
            "Time elapsed: 1.97 min\n",
            "Epoch: 012/015 | Batch 000/157 | Cost: 0.3330\n",
            "Epoch: 012/015 | Batch 050/157 | Cost: 0.2543\n",
            "Epoch: 012/015 | Batch 100/157 | Cost: 0.3131\n",
            "Epoch: 012/015 | Batch 150/157 | Cost: 0.3131\n",
            "training accuracy: 88.50%\n",
            "valid accuracy: 83.88%\n",
            "Time elapsed: 2.15 min\n",
            "Epoch: 013/015 | Batch 000/157 | Cost: 0.3078\n",
            "Epoch: 013/015 | Batch 050/157 | Cost: 0.2364\n",
            "Epoch: 013/015 | Batch 100/157 | Cost: 0.3307\n",
            "Epoch: 013/015 | Batch 150/157 | Cost: 0.3450\n",
            "training accuracy: 88.98%\n",
            "valid accuracy: 84.46%\n",
            "Time elapsed: 2.33 min\n",
            "Epoch: 014/015 | Batch 000/157 | Cost: 0.3768\n",
            "Epoch: 014/015 | Batch 050/157 | Cost: 0.3199\n",
            "Epoch: 014/015 | Batch 100/157 | Cost: 0.2975\n",
            "Epoch: 014/015 | Batch 150/157 | Cost: 0.2078\n",
            "training accuracy: 89.34%\n",
            "valid accuracy: 85.16%\n",
            "Time elapsed: 2.51 min\n",
            "Epoch: 015/015 | Batch 000/157 | Cost: 0.3260\n",
            "Epoch: 015/015 | Batch 050/157 | Cost: 0.2405\n",
            "Epoch: 015/015 | Batch 100/157 | Cost: 0.2861\n",
            "Epoch: 015/015 | Batch 150/157 | Cost: 0.3008\n",
            "training accuracy: 90.47%\n",
            "valid accuracy: 84.68%\n",
            "Time elapsed: 2.69 min\n",
            "Total Training Time: 2.69 min\n",
            "Test accuracy: 84.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Demo"
      ],
      "metadata": {
        "id": "qFbaT-VT3dkB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def predict_sentiment(model, sentence):\n",
        "    # based on:\n",
        "    # https://github.com/bentrevett/pytorch-sentiment-analysis/blob/\n",
        "    # master/2%20-%20Upgraded%20Sentiment%20Analysis.ipynb\n",
        "    model.eval()\n",
        "    tokenized = [tok.text for tok in nlp.tokenizer(sentence)]\n",
        "    indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
        "    length = [len(indexed)]\n",
        "    tensor = torch.LongTensor(indexed).to(DEVICE)\n",
        "    tensor = tensor.unsqueeze(1)\n",
        "    length_tensor = torch.LongTensor(length)\n",
        "    prediction = torch.sigmoid(model(tensor, length_tensor))\n",
        "    return prediction.item()"
      ],
      "metadata": {
        "id": "9eRuNxcl3fm2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Probability positive:')\n",
        "predict_sentiment(model, \"I really love this movie. This movie is so great!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zQ92GQCH3gcV",
        "outputId": "1378f104-4f75-42b8-890c-b82405853afc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probability positive:\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.842251181602478"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3 - Multilayer bidirectional RNN + LSTM"
      ],
      "metadata": {
        "id": "L7PaTMm-5mu5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import lib"
      ],
      "metadata": {
        "id": "2kQK45oh5tFj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1"
      ],
      "metadata": {
        "id": "3-46jZCy5oJl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "qf8OWmao54uL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT = data.Field(tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
        "                                          split_ratio=0.8)\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WvXTpvdN5vdb",
        "outputId": "8549693c-e0b5-4412-806e-ca88d2e8e8d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 20000\n",
            "Num Valid: 5000\n",
            "Num Test: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpxaAiQ-5ynx",
        "outputId": "9f8d21a0-ef32-408e-c82c-3b58260a52b6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20002\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    device=DEVICE)"
      ],
      "metadata": {
        "id": "dF8_jfdX50Sj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Hk56OXtg5187",
        "outputId": "a0737dcc-d709-4cae-9553-baf10ba2a169"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([132, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([59, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([42, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "VCGsfM8c6CZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, input_dim, embedding_dim, hidden_dim, output_dim):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
        "        self.rnn = nn.LSTM(embedding_dim,\n",
        "                           hidden_dim,\n",
        "                           num_layers=NUM_LAYERS,\n",
        "                           bidirectional=BIDIRECTIONAL)\n",
        "        self.fc = nn.Linear(hidden_dim*2, output_dim)\n",
        "        \n",
        "    def forward(self, text, text_length):\n",
        "\n",
        "        #[sentence len, batch size] => [sentence len, batch size, embedding size]\n",
        "        embedded = self.embedding(text)\n",
        "        \n",
        "        packed = torch.nn.utils.rnn.pack_padded_sequence(embedded, text_length.to('cpu'))\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.rnn(packed)\n",
        "        \n",
        "        # combine both directions\n",
        "        combined = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        \n",
        "        return self.fc(combined.squeeze(0)).view(-1)"
      ],
      "metadata": {
        "id": "IbftS3wG6FQm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM)\n",
        "model = model.to(DEVICE)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "GQqnzFMq6F4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model "
      ],
      "metadata": {
        "id": "RGSRWf1M6JRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_binary_accuracy(model, data_loader, device):\n",
        "    model.eval()\n",
        "    correct_pred, num_examples = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for batch_idx, batch_data in enumerate(data_loader):\n",
        "            text, text_lengths = batch_data.text\n",
        "            logits = model(text, text_lengths)\n",
        "            predicted_labels = (torch.sigmoid(logits) > 0.5).long()\n",
        "            num_examples += batch_data.label.size(0)\n",
        "            correct_pred += (predicted_labels == batch_data.label.long()).sum()\n",
        "        return correct_pred.float()/num_examples * 100"
      ],
      "metadata": {
        "id": "sqqzmNDr6Lz4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    for batch_idx, batch_data in enumerate(train_loader):\n",
        "        \n",
        "        text, text_lengths = batch_data.text\n",
        "        \n",
        "        ### FORWARD AND BACK PROP\n",
        "        logits = model(text, text_lengths)\n",
        "        cost = F.binary_cross_entropy_with_logits(logits, batch_data.label)\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        cost.backward()\n",
        "        \n",
        "        ### UPDATE MODEL PARAMETERS\n",
        "        optimizer.step()\n",
        "        \n",
        "        ### LOGGING\n",
        "        if not batch_idx % 50:\n",
        "            print (f'Epoch: {epoch+1:03d}/{NUM_EPOCHS:03d} | '\n",
        "                   f'Batch {batch_idx:03d}/{len(train_loader):03d} | '\n",
        "                   f'Cost: {cost:.4f}')\n",
        "\n",
        "    with torch.set_grad_enabled(False):\n",
        "        print(f'training accuracy: '\n",
        "              f'{compute_binary_accuracy(model, train_loader, DEVICE):.2f}%'\n",
        "              f'\\nvalid accuracy: '\n",
        "              f'{compute_binary_accuracy(model, valid_loader, DEVICE):.2f}%')\n",
        "        \n",
        "    print(f'Time elapsed: {(time.time() - start_time)/60:.2f} min')\n",
        "    \n",
        "print(f'Total Training Time: {(time.time() - start_time)/60:.2f} min')\n",
        "print(f'Test accuracy: {compute_binary_accuracy(model, test_loader, DEVICE):.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HO91L8KF6MNZ",
        "outputId": "5eb3639a-4382-4b72-95d7-0bb2bcfd0964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 001/015 | Batch 000/157 | Cost: 0.6932\n",
            "Epoch: 001/015 | Batch 050/157 | Cost: 0.6960\n",
            "Epoch: 001/015 | Batch 100/157 | Cost: 0.6795\n",
            "Epoch: 001/015 | Batch 150/157 | Cost: 0.6831\n",
            "training accuracy: 58.94%\n",
            "valid accuracy: 57.70%\n",
            "Time elapsed: 0.30 min\n",
            "Epoch: 002/015 | Batch 000/157 | Cost: 0.6762\n",
            "Epoch: 002/015 | Batch 050/157 | Cost: 0.6331\n",
            "Epoch: 002/015 | Batch 100/157 | Cost: 0.5971\n",
            "Epoch: 002/015 | Batch 150/157 | Cost: 0.5944\n",
            "training accuracy: 69.57%\n",
            "valid accuracy: 69.92%\n",
            "Time elapsed: 0.60 min\n",
            "Epoch: 003/015 | Batch 000/157 | Cost: 0.6172\n",
            "Epoch: 003/015 | Batch 050/157 | Cost: 0.5505\n",
            "Epoch: 003/015 | Batch 100/157 | Cost: 0.4957\n",
            "Epoch: 003/015 | Batch 150/157 | Cost: 0.6229\n",
            "training accuracy: 75.52%\n",
            "valid accuracy: 74.62%\n",
            "Time elapsed: 0.91 min\n",
            "Epoch: 004/015 | Batch 000/157 | Cost: 0.4683\n",
            "Epoch: 004/015 | Batch 050/157 | Cost: 0.5007\n",
            "Epoch: 004/015 | Batch 100/157 | Cost: 0.5525\n",
            "Epoch: 004/015 | Batch 150/157 | Cost: 0.5566\n",
            "training accuracy: 79.42%\n",
            "valid accuracy: 77.62%\n",
            "Time elapsed: 1.21 min\n",
            "Epoch: 005/015 | Batch 000/157 | Cost: 0.4884\n",
            "Epoch: 005/015 | Batch 050/157 | Cost: 0.4461\n",
            "Epoch: 005/015 | Batch 100/157 | Cost: 0.4264\n",
            "Epoch: 005/015 | Batch 150/157 | Cost: 0.4104\n",
            "training accuracy: 82.43%\n",
            "valid accuracy: 79.60%\n",
            "Time elapsed: 1.51 min\n",
            "Epoch: 006/015 | Batch 000/157 | Cost: 0.4178\n",
            "Epoch: 006/015 | Batch 050/157 | Cost: 0.3338\n",
            "Epoch: 006/015 | Batch 100/157 | Cost: 0.3174\n",
            "Epoch: 006/015 | Batch 150/157 | Cost: 0.4610\n",
            "training accuracy: 81.73%\n",
            "valid accuracy: 78.78%\n",
            "Time elapsed: 1.81 min\n",
            "Epoch: 007/015 | Batch 000/157 | Cost: 0.3570\n",
            "Epoch: 007/015 | Batch 050/157 | Cost: 0.3962\n",
            "Epoch: 007/015 | Batch 100/157 | Cost: 0.6669\n",
            "Epoch: 007/015 | Batch 150/157 | Cost: 0.2734\n",
            "training accuracy: 86.42%\n",
            "valid accuracy: 82.36%\n",
            "Time elapsed: 2.11 min\n",
            "Epoch: 008/015 | Batch 000/157 | Cost: 0.3151\n",
            "Epoch: 008/015 | Batch 050/157 | Cost: 0.3776\n",
            "Epoch: 008/015 | Batch 100/157 | Cost: 0.2750\n",
            "Epoch: 008/015 | Batch 150/157 | Cost: 0.2954\n",
            "training accuracy: 88.06%\n",
            "valid accuracy: 83.88%\n",
            "Time elapsed: 2.41 min\n",
            "Epoch: 009/015 | Batch 000/157 | Cost: 0.3604\n",
            "Epoch: 009/015 | Batch 050/157 | Cost: 0.4152\n",
            "Epoch: 009/015 | Batch 100/157 | Cost: 0.3092\n",
            "Epoch: 009/015 | Batch 150/157 | Cost: 0.3893\n",
            "training accuracy: 84.53%\n",
            "valid accuracy: 81.16%\n",
            "Time elapsed: 2.71 min\n",
            "Epoch: 010/015 | Batch 000/157 | Cost: 0.4177\n",
            "Epoch: 010/015 | Batch 050/157 | Cost: 0.2915\n",
            "Epoch: 010/015 | Batch 100/157 | Cost: 0.3627\n",
            "Epoch: 010/015 | Batch 150/157 | Cost: 0.2835\n",
            "training accuracy: 90.27%\n",
            "valid accuracy: 84.64%\n",
            "Time elapsed: 3.01 min\n",
            "Epoch: 011/015 | Batch 000/157 | Cost: 0.2572\n",
            "Epoch: 011/015 | Batch 050/157 | Cost: 0.2370\n",
            "Epoch: 011/015 | Batch 100/157 | Cost: 0.2393\n",
            "Epoch: 011/015 | Batch 150/157 | Cost: 0.2547\n",
            "training accuracy: 90.70%\n",
            "valid accuracy: 84.80%\n",
            "Time elapsed: 3.31 min\n",
            "Epoch: 012/015 | Batch 000/157 | Cost: 0.2649\n",
            "Epoch: 012/015 | Batch 050/157 | Cost: 0.1614\n",
            "Epoch: 012/015 | Batch 100/157 | Cost: 0.2026\n",
            "Epoch: 012/015 | Batch 150/157 | Cost: 0.2679\n",
            "training accuracy: 91.36%\n",
            "valid accuracy: 85.42%\n",
            "Time elapsed: 3.61 min\n",
            "Epoch: 013/015 | Batch 000/157 | Cost: 0.2828\n",
            "Epoch: 013/015 | Batch 050/157 | Cost: 0.2654\n",
            "Epoch: 013/015 | Batch 100/157 | Cost: 0.2395\n",
            "Epoch: 013/015 | Batch 150/157 | Cost: 0.2241\n",
            "training accuracy: 92.82%\n",
            "valid accuracy: 86.20%\n",
            "Time elapsed: 3.92 min\n",
            "Epoch: 014/015 | Batch 000/157 | Cost: 0.2491\n",
            "Epoch: 014/015 | Batch 050/157 | Cost: 0.2285\n",
            "Epoch: 014/015 | Batch 100/157 | Cost: 0.2154\n",
            "Epoch: 014/015 | Batch 150/157 | Cost: 0.1435\n",
            "training accuracy: 92.65%\n",
            "valid accuracy: 85.72%\n",
            "Time elapsed: 4.22 min\n",
            "Epoch: 015/015 | Batch 000/157 | Cost: 0.2398\n",
            "Epoch: 015/015 | Batch 050/157 | Cost: 0.1942\n",
            "Epoch: 015/015 | Batch 100/157 | Cost: 0.2408\n",
            "Epoch: 015/015 | Batch 150/157 | Cost: 0.2297\n",
            "training accuracy: 94.01%\n",
            "valid accuracy: 86.66%\n",
            "Time elapsed: 4.52 min\n",
            "Total Training Time: 4.52 min\n",
            "Test accuracy: 85.96%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4 - RNN + LSTM + Glob + dropout"
      ],
      "metadata": {
        "id": "BHp6KXtAvaeT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## import lib"
      ],
      "metadata": {
        "id": "VR7PUECwwM8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from spacy.tokenizer import Tokenizer\n",
        "# from torchtext import data\n",
        "# from torchtext import datasets\n",
        "from torchtext.legacy import data\n",
        "from torchtext.legacy import datasets\n",
        "\n",
        "# SEED = 11\n",
        "# torch.manual_seed(SEED)                         ## Reproducibility\n",
        "torch.backends.cudnn.deterministic = True\n",
        "\n",
        "RANDOM_SEED = 123\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "\n",
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "EMBEDDING_DIM = 128\n",
        "NUM_LAYERS = 2\n",
        "HIDDEN_DIM = 128\n",
        "OUTPUT_DIM = 1\n",
        "\n",
        "# TEXT = data.Field(tokenize = 'spacy', include_lengths = True)   ## Text field\n",
        "# LABEL = data.LabelField(dtype = torch.float)                    ## Label Field"
      ],
      "metadata": {
        "id": "3T5b8jYfvboW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preparing data"
      ],
      "metadata": {
        "id": "mtOX1s6ywPli"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "TEXT = data.Field(tokenize='spacy',\n",
        "                  include_lengths=True) # necessary for packed_padded_sequence\n",
        "LABEL = data.LabelField(dtype=torch.float)\n",
        "\n",
        "train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)\n",
        "train_data, valid_data = train_data.split(random_state=random.seed(RANDOM_SEED),\n",
        "                                          split_ratio=0.8)\n",
        "\n",
        "print(f'Num Train: {len(train_data)}')\n",
        "print(f'Num Valid: {len(valid_data)}')\n",
        "print(f'Num Test: {len(test_data)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94ZGed6Uv0YU",
        "outputId": "82f5f6f5-11a6-424c-a36b-9581f5f8a71a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torchtext/data/utils.py:123: UserWarning: Spacy model \"en\" could not be loaded, trying \"en_core_web_sm\" instead\n",
            "  warnings.warn(f'Spacy model \"{language}\" could not be loaded, trying \"{OLD_MODEL_SHORTCUTS[language]}\" instead')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "downloading aclImdb_v1.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "aclImdb_v1.tar.gz: 100%|██████████| 84.1M/84.1M [00:01<00:00, 51.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num Train: 20000\n",
            "Num Valid: 5000\n",
            "Num Test: 25000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.build_vocab(train_data, max_size=VOCABULARY_SIZE, vectors='glove.6B.100d', unk_init=torch.Tensor.normal_)\n",
        "LABEL.build_vocab(train_data)\n",
        "\n",
        "print(f'Vocabulary size: {len(TEXT.vocab)}')\n",
        "print(f'Number of classes: {len(LABEL.vocab)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DkjUqncQxC6y",
        "outputId": "49a18858-fb7f-4bf8-cc28-85a767f1f6b4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:13<00:00, 29088.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 20002\n",
            "Number of classes: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, valid_loader, test_loader = data.BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data), \n",
        "    batch_size=BATCH_SIZE,\n",
        "    sort_within_batch=True, # necessary for packed_padded_sequence\n",
        "    device=DEVICE)"
      ],
      "metadata": {
        "id": "krBggLgpPGLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train')\n",
        "for batch in train_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nValid:')\n",
        "for batch in valid_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break\n",
        "    \n",
        "print('\\nTest:')\n",
        "for batch in test_loader:\n",
        "    print(f'Text matrix size: {batch.text[0].size()}')\n",
        "    print(f'Target vector size: {batch.label.size()}')\n",
        "    break"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8cWkIThtPRFl",
        "outputId": "99a822b6-7117-4009-ed17-a2c59e129732"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train\n",
            "Text matrix size: torch.Size([133, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Valid:\n",
            "Text matrix size: torch.Size([61, 128])\n",
            "Target vector size: torch.Size([128])\n",
            "\n",
            "Test:\n",
            "Text matrix size: torch.Size([42, 128])\n",
            "Target vector size: torch.Size([128])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build the model"
      ],
      "metadata": {
        "id": "olYGq3djxpyy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, \n",
        "                 n_layers, bidirectional, dropout, pad_idx):\n",
        "        \n",
        "        super().__init__()\n",
        "        \n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx = pad_idx)\n",
        "        \n",
        "        self.lstm = nn.LSTM(embedding_dim, \n",
        "                           hidden_dim, \n",
        "                           num_layers = n_layers, \n",
        "                           bidirectional = bidirectional, \n",
        "                           dropout = dropout)\n",
        "        \n",
        "        self.fc = nn.Linear(hidden_dim * 2, output_dim)\n",
        "        \n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        \n",
        "    def forward(self, text, text_lengths):\n",
        "        \n",
        "        embedding = self.embedding(text)    ## shape = (sent_length, batch_size)\n",
        "        embedded = self.dropout(embedding)  ## shape = (sent_length, batch_size, emb_dim)\n",
        "        \n",
        "        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, text_lengths)    ## pack sequence\n",
        "        \n",
        "        packed_output, (hidden, cell) = self.lstm(packed_embedded)\n",
        "        output, output_lengths = nn.utils.rnn.pad_packed_sequence(packed_output)        ## unpack sequence\n",
        "\n",
        "        ## output shape = (sent_len, batch_size, hid_dim * num_directions)\n",
        "        ## output over padding tokens are zero tensors\n",
        "        \n",
        "        ## hidden shape = (num_layers * num_directions, batch_size, hid_dim)\n",
        "        ## cell shape = (num_layers * num_directions, batch_size, hid_dim)\n",
        "        \n",
        "        ## concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers\n",
        "        ## and apply dropout\n",
        "        \n",
        "        hidden = self.dropout(torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim = 1)) ## shape = (batch_size, hid_dim * num_directions)\n",
        "            \n",
        "        return self.fc(hidden)"
      ],
      "metadata": {
        "id": "r2cjEtIbxSLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCABULARY_SIZE = 20000\n",
        "LEARNING_RATE = 1e-4\n",
        "BATCH_SIZE = 128\n",
        "NUM_EPOCHS = 15\n",
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "BIDIRECTIONAL = True\n",
        "\n",
        "EMBEDDING_DIM = 100  # 128\n",
        "NUM_LAYERS = 2 \n",
        "HIDDEN_DIM = 128 # 256\n",
        "OUTPUT_DIM = 1\n",
        "DROPOUT = 0.4\n",
        "PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]"
      ],
      "metadata": {
        "id": "ZfPEQVklx7Aw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "INPUT_DIM = len(TEXT.vocab)\n",
        "\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "model = Model(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, NUM_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX).to(DEVICE)\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "hE7do5ucMpb-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"There are {train_params} trainable parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCgbiszmxul_",
        "outputId": "8f6a6c47-d8f0-48cc-de3e-e17824bdc3d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2631241 trainable parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##  Replace initial embedding with pretrained embedding"
      ],
      "metadata": {
        "id": "47-nD9b2NgD7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "TEXT.vocab.vectors.size()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ITm7jwPuNw3X",
        "outputId": "370b5b8e-3ebf-4128-9296-8696e49f17c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([20002, 100])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pretrained_embeddings = TEXT.vocab.vectors\n",
        "model.embedding.weight.data.copy_(pretrained_embeddings)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dsOgr5PYNRAT",
        "outputId": "a1784a65-d1fa-43d5-a343-41e2e8dde1c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.3374, -0.1778, -0.3035,  ...,  0.2770,  0.6455, -0.8957],\n",
              "        [ 0.4928, -0.0141, -0.2747,  ...,  0.0493,  0.8484,  0.4671],\n",
              "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
              "        ...,\n",
              "        [-0.4098,  1.0487, -0.2304,  ..., -0.6889, -0.8636, -0.6693],\n",
              "        [-0.5235,  0.8734,  0.6664,  ...,  0.3166,  0.0621,  0.3844],\n",
              "        [ 0.0667,  0.3963, -0.5351,  ..., -0.5661,  0.4373,  0.5835]],\n",
              "       device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Replace and with zeros (they were initialized with the normal distribution)"
      ],
      "metadata": {
        "id": "DfBEnO2gOESY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
        "\n",
        "model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)\n",
        "\n",
        "print(model.embedding.weight.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q8Pr7GpVOAZC",
        "outputId": "e8ee8564-1b5d-473f-dc31-af9d7703122e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],\n",
            "        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],\n",
            "        ...,\n",
            "        [-0.4098,  1.0487, -0.2304,  ..., -0.6889, -0.8636, -0.6693],\n",
            "        [-0.5235,  0.8734,  0.6664,  ...,  0.3166,  0.0621,  0.3844],\n",
            "        [ 0.0667,  0.3963, -0.5351,  ..., -0.5661,  0.4373,  0.5835]],\n",
            "       device='cuda:0')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Train the model"
      ],
      "metadata": {
        "id": "aNy7qzEqOLd2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torchtext.legacy import data\n",
        "\n",
        "from torchtext.legacy import datasets\n",
        "import time\n",
        "import random\n",
        "\n",
        "torch.backends.cudnn.deterministic = True"
      ],
      "metadata": {
        "id": "Xl1cxluCOrGS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def binary_accuracy(preds, y):\n",
        "\n",
        "    rounded_preds = torch.round(torch.sigmoid(preds))\n",
        "    correct = (rounded_preds == y).float() #convert into float for division \n",
        "    accuracy = correct.sum() / len(correct)\n",
        "    return accuracy\n",
        "    \n",
        "def binary_classification_metrics(prediction, ground_truth):\n",
        "    '''\n",
        "    Computes metrics for binary classification\n",
        "\n",
        "    Arguments:\n",
        "    prediction, np array of bool (num_samples) - model predictions\n",
        "    ground_truth, np array of bool (num_samples) - true labels\n",
        "\n",
        "    Returns:\n",
        "    precision, recall, f1, accuracy - classification metrics\n",
        "    '''\n",
        "\n",
        "    prediction = torch.round(torch.sigmoid(prediction))\n",
        "    correct = (prediction == ground_truth).float() #convert into float for division \n",
        "    \n",
        "    precision = 0\n",
        "    recall = 0\n",
        "    accuracy = 0\n",
        "    f1 = 0\n",
        "\n",
        "    tp = 0      ## true positive\n",
        "    tn = 0      ## true negative\n",
        "    fp = 0      ## false positive\n",
        "    fn = 0      ## false negative\n",
        "\n",
        "    for i in range(len(prediction)):\n",
        "        if prediction[i] == True and ground_truth[i] == True:\n",
        "            tp += 1\n",
        "        if prediction[i] == True and ground_truth[i] == False:\n",
        "            fp += 1\n",
        "        if prediction[i] == False and ground_truth[i] == True:\n",
        "            fn += 1\n",
        "        if prediction[i] == False and ground_truth[i] == False:\n",
        "            tn += 1\n",
        "\n",
        "    accuracy = (tp + tn)/(tp + tn + fp + fn)\n",
        "    precision = tp/(tp + fp)\n",
        "    recall = tp/(tp + fn)\n",
        "    f1 = 2 * (precision * recall)/(precision + recall)\n",
        "\n",
        "    return precision, recall, f1, accuracy"
      ],
      "metadata": {
        "id": "RfO0RsiMOXzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(model, iterator, optimizer, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    \n",
        "    model.train()\n",
        "    for batch in iterator:\n",
        "        \n",
        "        optimizer.zero_grad()\n",
        "        text, text_lengths = batch.text\n",
        "        predictions = model(text, text_lengths.to('cpu')).squeeze(1)\n",
        "\n",
        "        loss = criterion(predictions, batch.label)\n",
        "        accuracy = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_accuracy += accuracy.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
      ],
      "metadata": {
        "id": "wao82nvYOI8Q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_accuracy = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths.to('cpu')).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            accuracy = binary_accuracy(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_accuracy += accuracy.item()\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_accuracy / len(iterator)"
      ],
      "metadata": {
        "id": "yWU5b8ZJOOAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def metrics(model, iterator, criterion):\n",
        "    \n",
        "    epoch_loss = 0\n",
        "    epoch_f1 = 0\n",
        "\n",
        "    tp = tn = fp = fn = 0\n",
        "    \n",
        "    model.eval()\n",
        "    \n",
        "    with torch.no_grad():\n",
        "        for batch in iterator:\n",
        "\n",
        "            text, text_lengths = batch.text\n",
        "            predictions = model(text, text_lengths.to('cpu')).squeeze(1)\n",
        "            loss = criterion(predictions, batch.label)\n",
        "            \n",
        "            precision, recall, f1, accuracy = binary_classification_metrics(predictions, batch.label)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_f1 += f1\n",
        "        \n",
        "    return epoch_loss / len(iterator), epoch_f1 / len(iterator)"
      ],
      "metadata": {
        "id": "JDZ0XKAVQDPK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
      ],
      "metadata": {
        "id": "7seqGI1TQFVy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.BCEWithLogitsLoss()      ## use GPU\n",
        "criterion = criterion.to(DEVICE)"
      ],
      "metadata": {
        "id": "hmiX-xjlQJbR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def epoch_time(start_time, end_time):\n",
        "    elapsed_time = end_time - start_time\n",
        "    elapsed_mins = int(elapsed_time / 60)\n",
        "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
        "    return elapsed_mins, elapsed_secs"
      ],
      "metadata": {
        "id": "D_vuejn1QcJw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_valid_loss = float('inf')\n",
        "\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "\n",
        "    start_time = time.time()\n",
        "    \n",
        "    train_loss, train_accuracy = train(model, train_loader, optimizer, criterion)\n",
        "    valid_loss, valid_accuracy = evaluate(model, valid_loader, criterion)\n",
        "    \n",
        "    end_time = time.time()\n",
        "\n",
        "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
        "    \n",
        "    if valid_loss < best_valid_loss:\n",
        "        best_valid_loss = valid_loss\n",
        "        torch.save(model, 'model.pt')\n",
        "    \n",
        "    print(f'Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_accuracy*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_accuracy*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TL5f2qA6QNTI",
        "outputId": "20cfdc10-e8a4-4d12-9537-f3579d68d681"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.669 | Train Acc: 58.55%\n",
            "\t Val. Loss: 0.688 |  Val. Acc: 56.11%\n",
            "Epoch: 02 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.623 | Train Acc: 65.27%\n",
            "\t Val. Loss: 0.571 |  Val. Acc: 69.20%\n",
            "Epoch: 03 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.640 | Train Acc: 62.55%\n",
            "\t Val. Loss: 0.600 |  Val. Acc: 68.36%\n",
            "Epoch: 04 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.478 | Train Acc: 77.66%\n",
            "\t Val. Loss: 0.522 |  Val. Acc: 72.66%\n",
            "Epoch: 05 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.382 | Train Acc: 83.91%\n",
            "\t Val. Loss: 0.321 |  Val. Acc: 87.13%\n",
            "Epoch: 06 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.383 | Train Acc: 84.36%\n",
            "\t Val. Loss: 0.748 |  Val. Acc: 61.05%\n",
            "Epoch: 07 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.503 | Train Acc: 75.05%\n",
            "\t Val. Loss: 0.357 |  Val. Acc: 84.71%\n",
            "Epoch: 08 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.289 | Train Acc: 88.44%\n",
            "\t Val. Loss: 0.287 |  Val. Acc: 88.44%\n",
            "Epoch: 09 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.252 | Train Acc: 89.86%\n",
            "\t Val. Loss: 0.362 |  Val. Acc: 83.32%\n",
            "Epoch: 10 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.246 | Train Acc: 90.52%\n",
            "\t Val. Loss: 0.257 |  Val. Acc: 89.75%\n",
            "Epoch: 11 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.207 | Train Acc: 92.00%\n",
            "\t Val. Loss: 0.278 |  Val. Acc: 88.85%\n",
            "Epoch: 12 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.181 | Train Acc: 93.23%\n",
            "\t Val. Loss: 0.257 |  Val. Acc: 89.69%\n",
            "Epoch: 13 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.167 | Train Acc: 93.96%\n",
            "\t Val. Loss: 0.285 |  Val. Acc: 89.82%\n",
            "Epoch: 14 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.156 | Train Acc: 94.18%\n",
            "\t Val. Loss: 0.266 |  Val. Acc: 89.73%\n",
            "Epoch: 15 | Epoch Time: 0m 16s\n",
            "\tTrain Loss: 0.139 | Train Acc: 94.93%\n",
            "\t Val. Loss: 0.262 |  Val. Acc: 90.76%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
        "\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z0k2yDqcRBqg",
        "outputId": "b373a0fa-5755-481a-aff7-23704d3a1e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.306 | Test Acc: 88.74%\n"
          ]
        }
      ]
    }
  ]
}